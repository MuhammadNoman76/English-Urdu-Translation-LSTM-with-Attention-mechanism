{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af81f13-a4a8-49c7-b95c-99bcea212403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Source Length: 18\n",
      "Max Target Lenght: 23\n",
      "16\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import re\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
    "from tensorflow.keras.layers import GRU , MultiHeadAttention, TimeDistributed ,Input, Dense, Input, Embedding, LSTM, Dense, Concatenate, AdditiveAttention , Bidirectional, concatenate , Dropout , Activation, dot, concatenate\n",
    "from tensorflow.keras.layers import Attention\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Attention, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "lines=pd.read_table('urdu.txt', names=['eng', 'urdu'],index_col=False,encoding = 'utf-16')\n",
    "\n",
    "\n",
    "lines.shape\n",
    "\n",
    "# Lowercase all characters\n",
    "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
    "lines.urdu=lines.urdu.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines.urdu=lines.urdu.apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "\n",
    "# Remove all the special characters\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.urdu=lines.urdu.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
    "lines.urdu=lines.urdu.apply(lambda x: x.strip())\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.urdu=lines.urdu.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "lines.urdu = lines.urdu.apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n",
    "lines.sample(10)\n",
    "\n",
    "\n",
    "# Vocabulary of English\n",
    "all_eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of Urdu \n",
    "all_urdu_words=set()\n",
    "for urdu in lines.urdu:\n",
    "    for word in urdu.split():\n",
    "        if word not in all_urdu_words:\n",
    "            all_urdu_words.add(word)\n",
    "\n",
    "\n",
    "\n",
    "# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "print('Max Source Length:',max_length_src)\n",
    "\n",
    "# Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in lines.urdu:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "print('Max Target Lenght:',max_length_tar)\n",
    "\n",
    "\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_urdu_words))\n",
    "num_encoder_tokens = len(all_eng_words)+1\n",
    "num_decoder_tokens = len(all_urdu_words)+1\n",
    "num_encoder_tokens, num_decoder_tokens\n",
    "\n",
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens\n",
    "\n",
    "\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "\n",
    "lines = shuffle(lines)\n",
    "lines.head(10)\n",
    "\n",
    "# Train - Test Split\n",
    "X, y = lines.eng, lines.urdu\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "#Save the train and test dataframes for reproducing the results later, as they are shuffled.\n",
    "\n",
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')\n",
    "\n",
    "\n",
    "def generate_batch(X = X_train, y = y_train, batch_size = 64):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "\n",
    "\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 128\n",
    "print(train_samples//batch_size)\n",
    "print(val_samples//batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6deaccd3-7feb-485a-b5b5-1f5eb4501796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    }
   ],
   "source": [
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = Attention(name='attention_layer')\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model with a RMSprop optimizer and a suitable learning rate\n",
    "rmsprop_optimizer = RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rmsprop_optimizer, loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18bd14c3-66e4-4b70-9f7d-f6da25ef0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4209d6-6673-4392-810f-c57f3f133b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Noman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Noman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Noman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Noman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 17s 303ms/step - loss: 7.4074 - acc: 0.1125 - val_loss: 7.3134 - val_acc: 0.1077\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 6.6312 - acc: 0.1214 - val_loss: 5.9359 - val_acc: 0.1253\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 5.8989 - acc: 0.1295 - val_loss: 5.9052 - val_acc: 0.1178\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 3s 207ms/step - loss: 5.7068 - acc: 0.1431 - val_loss: 5.5926 - val_acc: 0.1548\n",
      "Epoch 5/100\n",
      " 6/16 [==========>...................] - ETA: 2s - loss: 5.5902 - acc: 0.1507"
     ]
    }
   ],
   "source": [
    "model.fit(generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cf3e6-ae65-474d-bfee-20bda0c5e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('model_with_attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb9d5e-f0e6-4044-8327-019ec4f1b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models\n",
    "# For the encoder model, the outputs are the encoder outputs as well to be used in attention\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_states)\n",
    "\n",
    "# Decoder inference\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_length_src, latent_dim))\n",
    "\n",
    "# Get the embeddings and apply the decoder LSTM\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# Attention inference\n",
    "attention_result_inf = attention_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attention_result_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c47e1b-ee9f-4005-b6db-d6affdf32d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loop to decode a batch of sequences\n",
    "def decode_sequence_batch(input_seqs):\n",
    "    # Encode the input as state vectors\n",
    "    encoder_outputs, states_value_h, states_value_c = encoder_model.predict(input_seqs)\n",
    "\n",
    "    # Create batch size empty target sequences\n",
    "    target_seqs = np.zeros((input_seqs.shape[0], 1))\n",
    "    # Populate the first character of target sequence with the start token for each one\n",
    "    target_seqs[:, 0] = target_token_index['START_']\n",
    "\n",
    "    decoded_sentences = [''] * input_seqs.shape[0]\n",
    "    stop_conditions = [False] * input_seqs.shape[0]\n",
    "\n",
    "    while not all(stop_conditions):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seqs] + [encoder_outputs, states_value_h, states_value_c])\n",
    "\n",
    "        # Sample tokens for each sequence in the batch\n",
    "        sampled_token_indices = np.argmax(output_tokens[:, -1, :], axis=1)\n",
    "        for i in range(input_seqs.shape[0]):\n",
    "            if not stop_conditions[i]:\n",
    "                sampled_token = reverse_target_char_index[sampled_token_indices[i]]\n",
    "                decoded_sentences[i] += ' ' + sampled_token\n",
    "\n",
    "                # Check for stop condition: either hit max length or find stop token\n",
    "                if (sampled_token == '_END' or len(decoded_sentences[i]) > max_length_tar):\n",
    "                    stop_conditions[i] = True\n",
    "\n",
    "        # Update the target sequence\n",
    "        target_seqs = np.array(sampled_token_indices)[:, np.newaxis]\n",
    "\n",
    "        # Update states\n",
    "        states_value_h, states_value_c = h, c\n",
    "\n",
    "    return decoded_sentences\n",
    "\n",
    "# Define a function to convert raw text to padded sequences\n",
    "def text_to_padded_sequences(texts, tokenizer, maxlen):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Function to translate new sentences that are not in the dataset\n",
    "def translate_new_sentences(sentences):\n",
    "    preprocessed_sentences = preprocess_sentences(sentences) # You should define a function that preprocesses new sentences like the original data\n",
    "    input_seqs = text_to_padded_sequences(preprocessed_sentences, input_tokenizer, max_length_src)\n",
    "    decoded_sentences = decode_sequence_batch(input_seqs)\n",
    "    return decoded_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e1d14-2baa-48d4-be49-804224593d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "new_sentences = ['This is a new sentence.', 'Another example sentence.']\n",
    "translations = translate_new_sentences(new_sentences)\n",
    "for translation in translations:\n",
    "    print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb50e5f-4c5e-4266-8a7d-2633cb17ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode sample sequences function using attention\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors and retrieve encoder outputs\n",
    "    encoder_outputs, h, c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with the start token\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences (assuming a batch of size 1)\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [encoder_outputs, h, c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_char == '_END'):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence with the sampled token\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd9de5-fdbc-4ec3-b06a-1907b5ec403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1\n",
    "\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Urdu Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Urdu Translation:', decoded_sentence[:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
