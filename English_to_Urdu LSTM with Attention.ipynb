{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54abb3f7-d7d4-4f42-b375-8b9d0a4948f9",
   "metadata": {},
   "source": [
    "# Important Packages to Install Before Starting the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9886f-03f9-4607-aca5-54a748a7387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy matplotlib scikit-learn keras tensorflow gensim seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cceac87-a365-4d2b-9544-ef73b82f7d88",
   "metadata": {},
   "source": [
    "# Import All the Important Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a202cf-faf0-4c31-8fd9-91efc97e5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import re\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers , activations , models , preprocessing, utils\n",
    "from tensorflow.keras.layers import GRU , MultiHeadAttention, TimeDistributed ,Input, Dense, Input, Embedding, LSTM, Dense, Concatenate, AdditiveAttention , Bidirectional, concatenate , Dropout , Activation, dot, concatenate\n",
    "from tensorflow.keras.layers import Attention\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Attention, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478ac5b-868d-431d-9324-59b218ff342b",
   "metadata": {},
   "source": [
    "# Translation Files\n",
    "\n",
    "## Novel Book Translation\n",
    "\n",
    "- **Language**: English\n",
    "- **File Name**: urdu.txt\n",
    "- **Content**: The text of the novel in English.\n",
    "\n",
    "## Bible Translation\n",
    "\n",
    "- **Language**: Urdu\n",
    "- **File Name**: bible..xlsx\n",
    "- **Content**: The translation of the Bible in Urdu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694fc0c9-42e4-427b-8c28-1d147f87f124",
   "metadata": {},
   "source": [
    "## You Can Load Files Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8828a632-0d0c-4ccb-9856-b980927b6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines=pd.read_table('urdu.txt', names=['eng', 'urdu'],index_col=False,encoding = 'utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f42d6f48-21ef-424f-889e-3fddbbcfbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the Excel file\n",
    "# lines = pd.read_excel('cleaned_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65075c6a-4e60-417a-8455-0d50519335fe",
   "metadata": {},
   "source": [
    "## You Can also train by merging file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "080adc2d-fab6-4013-8820-6d307239e88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved as english_to_urdu_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "excel_data = pd.read_excel('bible.xlsx')\n",
    "\n",
    "# Read the text file\n",
    "text_data = pd.read_table('urdu.txt', names=['eng', 'urdu'], index_col=False, encoding='utf-16')\n",
    "\n",
    "# Merge the data\n",
    "merged_data = pd.concat([excel_data, text_data])\n",
    "\n",
    "# Optionally, reset index\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the merged data as Excel file\n",
    "merged_data.to_excel('english_to_urdu_dataset.xlsx', index=False)\n",
    "\n",
    "print(\"Merged data saved as english_to_urdu_dataset.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b576441-2070-4751-aace-1d489706086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "lines = pd.read_excel('english_to_urdu_dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e88b7-f701-4992-84bd-c378e65de7e7",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6d93e06-f5cc-430b-9dd0-abe5dc0f1c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>urdu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>and when the disciples heard it they fell on t...</td>\n",
       "      <td>START_ شاگرد یہ سن کر منہ کے بل گرے اور بہت سے...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4877</th>\n",
       "      <td>do we then make void the law through faith god...</td>\n",
       "      <td>START_ پس کیا ہم شریعت کو ایمان سے باطل کرتے ہ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6608</th>\n",
       "      <td>if thou put the brethren in remembrance of the...</td>\n",
       "      <td>START_ اگر تو بھائیوں کو یہ باتیں یاد دلائے گا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>and again he entered into capernaum after some...</td>\n",
       "      <td>START_ کئی دن بعد جب وہ کفرنحوم میں پھر داخل ہ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>and said behold i see the heavens opened and t...</td>\n",
       "      <td>START_ کہا کہ دیکھو ۔ میں آسمان کو کھلا اور اب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>and when he sowed some seeds fell by the way s...</td>\n",
       "      <td>START_ اور بوتے وقت کچھ دانے راہ کے کنارے گرے ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>have all the gifts of healing do all speak wit...</td>\n",
       "      <td>START_ کیا سب کو شفا دینے کی قوّت عنایت ہوئی ک...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>saying the son of man must be delivered into t...</td>\n",
       "      <td>START_ ضرور ہے کہ ابن آدم گنہگاروں کے ہاتھ میں...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>for this melchisedec king of salem priest of t...</td>\n",
       "      <td>START_ اور یہ ملک صدق سالم کا بادشاہ ۔ خدا تعا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8500</th>\n",
       "      <td>thats not the right answer</td>\n",
       "      <td>START_ یہ صحیح جواب نہیں ہے۔ _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    eng  \\\n",
       "561   and when the disciples heard it they fell on t...   \n",
       "4877  do we then make void the law through faith god...   \n",
       "6608  if thou put the brethren in remembrance of the...   \n",
       "1116  and again he entered into capernaum after some...   \n",
       "4027  and said behold i see the heavens opened and t...   \n",
       "398   and when he sowed some seeds fell by the way s...   \n",
       "5519  have all the gifts of healing do all speak wit...   \n",
       "2853  saying the son of man must be delivered into t...   \n",
       "6920  for this melchisedec king of salem priest of t...   \n",
       "8500                         thats not the right answer   \n",
       "\n",
       "                                                   urdu  \n",
       "561   START_ شاگرد یہ سن کر منہ کے بل گرے اور بہت سے...  \n",
       "4877  START_ پس کیا ہم شریعت کو ایمان سے باطل کرتے ہ...  \n",
       "6608  START_ اگر تو بھائیوں کو یہ باتیں یاد دلائے گا...  \n",
       "1116  START_ کئی دن بعد جب وہ کفرنحوم میں پھر داخل ہ...  \n",
       "4027  START_ کہا کہ دیکھو ۔ میں آسمان کو کھلا اور اب...  \n",
       "398   START_ اور بوتے وقت کچھ دانے راہ کے کنارے گرے ...  \n",
       "5519  START_ کیا سب کو شفا دینے کی قوّت عنایت ہوئی ک...  \n",
       "2853  START_ ضرور ہے کہ ابن آدم گنہگاروں کے ہاتھ میں...  \n",
       "6920  START_ اور یہ ملک صدق سالم کا بادشاہ ۔ خدا تعا...  \n",
       "8500                  START_ یہ صحیح جواب نہیں ہے۔ _END  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert numerical values to strings\n",
    "lines['urdu'] = lines['urdu'].astype(str)\n",
    "\n",
    "# Lowercase all characters\n",
    "lines['eng'] = lines['eng'].apply(lambda x: x.lower())\n",
    "lines['urdu'] = lines['urdu'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "lines['eng'] = lines['eng'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines['urdu'] = lines['urdu'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "exclude = set(string.punctuation)  # Set of all special characters\n",
    "\n",
    "# Remove all the special characters\n",
    "lines['eng'] = lines['eng'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines['urdu'] = lines['urdu'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines['eng'] = lines['eng'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines['eng'] = lines['eng'].apply(lambda x: x.strip())\n",
    "lines['urdu'] = lines['urdu'].apply(lambda x: x.strip())\n",
    "lines['eng'] = lines['eng'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines['urdu'] = lines['urdu'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "lines['urdu'] = lines['urdu'].apply(lambda x: 'START_ ' + x + ' _END')\n",
    "\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1fb4a97e-4357-4db5-bc85-180b06d12c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of Urdu \n",
    "all_urdu_words=set()\n",
    "for urdu in lines.urdu:\n",
    "    for word in urdu.split():\n",
    "        if word not in all_urdu_words:\n",
    "            all_urdu_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2af56b52-b12c-45ed-b4b6-d5c7ccc8b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Source Length: 68\n",
      "Max Target Lenght: 86\n"
     ]
    }
   ],
   "source": [
    "# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "print('Max Source Length:',max_length_src)\n",
    "\n",
    "# Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in lines.urdu:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "print('Max Target Lenght:',max_length_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a559e4b-dfcf-4939-a52a-b7ba9cd99ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>urdu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>and if ye salute your brethren only what do ye...</td>\n",
       "      <td>START_ اور اگر تم فقط اپنے بھائیوں ہی کو سلام ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>as it is written in the book of the words of e...</td>\n",
       "      <td>START_ جیسا یسعیاہ نبی کے کلام کی کتاب میں لکھ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>but when the blade was sprung up and brought f...</td>\n",
       "      <td>START_ پس جب پتّیا نکلیں اور بالیں آئیں تو وہ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7829</th>\n",
       "      <td>and every island fled away and the mountains w...</td>\n",
       "      <td>START_ اور ہر ایک ٹاپو اپنی جگہ سے ٹل گیا اور ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>for verily i say unto you that many prophets a...</td>\n",
       "      <td>START_ کیونکہ میں تم سے سچ کہتا ہوں کہ بہت سے ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>is it a recent picture</td>\n",
       "      <td>START_ یہ ایک حالیہ تصویر ہے؟ _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>but peter said ananias why hath satan filled t...</td>\n",
       "      <td>START_ مگر پطرس نے کہا اے حننیاہ ۔ کیوں شیطان ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>and he said unto them what manner of communica...</td>\n",
       "      <td>START_ اس نے ان سے کہا یہ کیا باتیں ہیں جو تم ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6978</th>\n",
       "      <td>whereupon neither the first testament was dedi...</td>\n",
       "      <td>START_ اسی لئے پہلا عہد بھی بغیر خون کے نہیں ب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5783</th>\n",
       "      <td>wherefore though i wrote unto you i did it not...</td>\n",
       "      <td>START_ پس اگرچہ میں نے تم کو لکھا تھا مگر نہ ا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    eng  \\\n",
       "136   and if ye salute your brethren only what do ye...   \n",
       "1884  as it is written in the book of the words of e...   \n",
       "420   but when the blade was sprung up and brought f...   \n",
       "7829  and every island fled away and the mountains w...   \n",
       "411   for verily i say unto you that many prophets a...   \n",
       "8300                             is it a recent picture   \n",
       "3917  but peter said ananias why hath satan filled t...   \n",
       "2863  and he said unto them what manner of communica...   \n",
       "6978  whereupon neither the first testament was dedi...   \n",
       "5783  wherefore though i wrote unto you i did it not...   \n",
       "\n",
       "                                                   urdu  \n",
       "136   START_ اور اگر تم فقط اپنے بھائیوں ہی کو سلام ...  \n",
       "1884  START_ جیسا یسعیاہ نبی کے کلام کی کتاب میں لکھ...  \n",
       "420   START_ پس جب پتّیا نکلیں اور بالیں آئیں تو وہ ...  \n",
       "7829  START_ اور ہر ایک ٹاپو اپنی جگہ سے ٹل گیا اور ...  \n",
       "411   START_ کیونکہ میں تم سے سچ کہتا ہوں کہ بہت سے ...  \n",
       "8300                 START_ یہ ایک حالیہ تصویر ہے؟ _END  \n",
       "3917  START_ مگر پطرس نے کہا اے حننیاہ ۔ کیوں شیطان ...  \n",
       "2863  START_ اس نے ان سے کہا یہ کیا باتیں ہیں جو تم ...  \n",
       "6978  START_ اسی لئے پہلا عہد بھی بغیر خون کے نہیں ب...  \n",
       "5783  START_ پس اگرچہ میں نے تم کو لکھا تھا مگر نہ ا...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_urdu_words))\n",
    "num_encoder_tokens = len(all_eng_words)+1\n",
    "num_decoder_tokens = len(all_urdu_words)+1\n",
    "num_encoder_tokens, num_decoder_tokens\n",
    "\n",
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens\n",
    "\n",
    "\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "\n",
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59336df-f39c-4ba8-809f-77eb329c7593",
   "metadata": {},
   "source": [
    "# Split Data into Training and Testing for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3853cb99-daeb-4b13-ae24-f5718acb0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train - Test Split\n",
    "X, y = lines.eng, lines.urdu\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "\n",
    "#Save the train and test dataframes for reproducing the results later, as they are shuffled.\n",
    "\n",
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2af81f13-a4a8-49c7-b95c-99bcea212403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 64):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dae208-2065-4b44-89da-88305770e104",
   "metadata": {},
   "source": [
    "# Set the Model Complexity According to Your Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1281136f-dcb0-4708-9bf5-6806d3a9e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 64\n",
    "epochs = 500\n",
    "latent_dim = 128\n",
    "print(train_samples//batch_size)\n",
    "print(val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09707f-8b88-40d1-a8e2-2e10e35dc9d8",
   "metadata": {},
   "source": [
    "# Prepared Model of LSTM with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6deaccd3-7feb-485a-b5b5-1f5eb4501796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    }
   ],
   "source": [
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = Attention(name='attention_layer')\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model with a RMSprop optimizer and a suitable learning rate\n",
    "rmsprop_optimizer = RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rmsprop_optimizer, loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18bd14c3-66e4-4b70-9f7d-f6da25ef0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4209d6-6673-4392-810f-c57f3f133b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "128/128 [==============================] - 268s 2s/step - loss: 6.5106 - acc: 0.0620 - val_loss: 5.9141 - val_acc: 0.0771\n",
      "Epoch 2/500\n",
      "128/128 [==============================] - 268s 2s/step - loss: 5.9049 - acc: 0.0783 - val_loss: 5.8795 - val_acc: 0.0830\n",
      "Epoch 3/500\n",
      "128/128 [==============================] - 289s 2s/step - loss: 5.8580 - acc: 0.0907 - val_loss: 5.8303 - val_acc: 0.1029\n",
      "Epoch 4/500\n",
      "128/128 [==============================] - 289s 2s/step - loss: 5.7796 - acc: 0.1108 - val_loss: 5.7503 - val_acc: 0.1112\n",
      "Epoch 5/500\n",
      "128/128 [==============================] - 280s 2s/step - loss: 5.7022 - acc: 0.1165 - val_loss: 5.6736 - val_acc: 0.1199\n",
      "Epoch 6/500\n",
      "128/128 [==============================] - 266s 2s/step - loss: 5.6086 - acc: 0.1245 - val_loss: 5.5788 - val_acc: 0.1280\n",
      "Epoch 7/500\n",
      "128/128 [==============================] - 266s 2s/step - loss: 5.5132 - acc: 0.1317 - val_loss: 5.4667 - val_acc: 0.1357\n",
      "Epoch 8/500\n",
      "128/128 [==============================] - 266s 2s/step - loss: 5.4137 - acc: 0.1381 - val_loss: 5.3716 - val_acc: 0.1414\n",
      "Epoch 9/500\n",
      "128/128 [==============================] - 270s 2s/step - loss: 5.3085 - acc: 0.1456 - val_loss: 5.2813 - val_acc: 0.1508\n",
      "Epoch 10/500\n",
      "128/128 [==============================] - 269s 2s/step - loss: 5.2154 - acc: 0.1540 - val_loss: 5.2041 - val_acc: 0.1593\n",
      "Epoch 11/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 5.1398 - acc: 0.1604 - val_loss: 5.1524 - val_acc: 0.1628\n",
      "Epoch 12/500\n",
      "128/128 [==============================] - 269s 2s/step - loss: 5.0764 - acc: 0.1663 - val_loss: 5.0824 - val_acc: 0.1694\n",
      "Epoch 13/500\n",
      "128/128 [==============================] - 266s 2s/step - loss: 5.0213 - acc: 0.1713 - val_loss: 5.0386 - val_acc: 0.1762\n",
      "Epoch 14/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.9691 - acc: 0.1763 - val_loss: 4.9983 - val_acc: 0.1814\n",
      "Epoch 15/500\n",
      "128/128 [==============================] - 271s 2s/step - loss: 4.9170 - acc: 0.1815 - val_loss: 4.9525 - val_acc: 0.1861\n",
      "Epoch 16/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.8658 - acc: 0.1869 - val_loss: 4.9009 - val_acc: 0.1905\n",
      "Epoch 17/500\n",
      "128/128 [==============================] - 266s 2s/step - loss: 4.8182 - acc: 0.1907 - val_loss: 4.8753 - val_acc: 0.1961\n",
      "Epoch 18/500\n",
      "128/128 [==============================] - 266s 2s/step - loss: 4.7759 - acc: 0.1945 - val_loss: 4.8514 - val_acc: 0.1983\n",
      "Epoch 19/500\n",
      "128/128 [==============================] - 268s 2s/step - loss: 4.7364 - acc: 0.1982 - val_loss: 4.8130 - val_acc: 0.2007\n",
      "Epoch 20/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.6996 - acc: 0.2010 - val_loss: 4.7877 - val_acc: 0.2025\n",
      "Epoch 21/500\n",
      "128/128 [==============================] - 266s 2s/step - loss: 4.6656 - acc: 0.2041 - val_loss: 4.7657 - val_acc: 0.2064\n",
      "Epoch 22/500\n",
      "128/128 [==============================] - 268s 2s/step - loss: 4.6305 - acc: 0.2068 - val_loss: 4.7231 - val_acc: 0.2094\n",
      "Epoch 23/500\n",
      "128/128 [==============================] - 269s 2s/step - loss: 4.5970 - acc: 0.2098 - val_loss: 4.7053 - val_acc: 0.2107\n",
      "Epoch 24/500\n",
      "128/128 [==============================] - 270s 2s/step - loss: 4.5625 - acc: 0.2136 - val_loss: 4.6912 - val_acc: 0.2139\n",
      "Epoch 25/500\n",
      "128/128 [==============================] - 272s 2s/step - loss: 4.5316 - acc: 0.2167 - val_loss: 4.6708 - val_acc: 0.2171\n",
      "Epoch 26/500\n",
      "128/128 [==============================] - 284s 2s/step - loss: 4.5014 - acc: 0.2194 - val_loss: 4.6589 - val_acc: 0.2193\n",
      "Epoch 27/500\n",
      "128/128 [==============================] - 313s 2s/step - loss: 4.4702 - acc: 0.2228 - val_loss: 4.6171 - val_acc: 0.2250\n",
      "Epoch 28/500\n",
      "128/128 [==============================] - 277s 2s/step - loss: 4.4412 - acc: 0.2255 - val_loss: 4.6063 - val_acc: 0.2264\n",
      "Epoch 29/500\n",
      "128/128 [==============================] - 271s 2s/step - loss: 4.4124 - acc: 0.2283 - val_loss: 4.5941 - val_acc: 0.2265\n",
      "Epoch 30/500\n",
      "128/128 [==============================] - 273s 2s/step - loss: 4.3850 - acc: 0.2312 - val_loss: 4.5706 - val_acc: 0.2294\n",
      "Epoch 31/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.3567 - acc: 0.2333 - val_loss: 4.5477 - val_acc: 0.2307\n",
      "Epoch 32/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.3324 - acc: 0.2352 - val_loss: 4.5429 - val_acc: 0.2338\n",
      "Epoch 33/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.3054 - acc: 0.2387 - val_loss: 4.5330 - val_acc: 0.2346\n",
      "Epoch 34/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.2803 - acc: 0.2411 - val_loss: 4.5154 - val_acc: 0.2354\n",
      "Epoch 35/500\n",
      "128/128 [==============================] - 268s 2s/step - loss: 4.2568 - acc: 0.2434 - val_loss: 4.4956 - val_acc: 0.2374\n",
      "Epoch 36/500\n",
      "128/128 [==============================] - 274s 2s/step - loss: 4.2318 - acc: 0.2457 - val_loss: 4.4855 - val_acc: 0.2405\n",
      "Epoch 37/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.2097 - acc: 0.2474 - val_loss: 4.4573 - val_acc: 0.2415\n",
      "Epoch 38/500\n",
      "128/128 [==============================] - 266s 2s/step - loss: 4.1863 - acc: 0.2500 - val_loss: 4.4533 - val_acc: 0.2423\n",
      "Epoch 39/500\n",
      "128/128 [==============================] - 326s 3s/step - loss: 4.1631 - acc: 0.2522 - val_loss: 4.4492 - val_acc: 0.2449\n",
      "Epoch 40/500\n",
      "128/128 [==============================] - 341s 3s/step - loss: 4.1419 - acc: 0.2540 - val_loss: 4.4394 - val_acc: 0.2432\n",
      "Epoch 41/500\n",
      "128/128 [==============================] - 283s 2s/step - loss: 4.1178 - acc: 0.2556 - val_loss: 4.4313 - val_acc: 0.2438\n",
      "Epoch 42/500\n",
      "128/128 [==============================] - 316s 2s/step - loss: 4.0975 - acc: 0.2573 - val_loss: 4.3982 - val_acc: 0.2490\n",
      "Epoch 43/500\n",
      "128/128 [==============================] - 270s 2s/step - loss: 4.0751 - acc: 0.2603 - val_loss: 4.3980 - val_acc: 0.2480\n",
      "Epoch 44/500\n",
      "128/128 [==============================] - 271s 2s/step - loss: 4.0548 - acc: 0.2619 - val_loss: 4.3958 - val_acc: 0.2473\n",
      "Epoch 45/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.0328 - acc: 0.2642 - val_loss: 4.3765 - val_acc: 0.2495\n",
      "Epoch 46/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 4.0127 - acc: 0.2658 - val_loss: 4.3570 - val_acc: 0.2516\n",
      "Epoch 47/500\n",
      "128/128 [==============================] - 282s 2s/step - loss: 3.9929 - acc: 0.2674 - val_loss: 4.3615 - val_acc: 0.2541\n",
      "Epoch 48/500\n",
      "128/128 [==============================] - 269s 2s/step - loss: 3.9725 - acc: 0.2696 - val_loss: 4.3528 - val_acc: 0.2524\n",
      "Epoch 49/500\n",
      "128/128 [==============================] - 279s 2s/step - loss: 3.9532 - acc: 0.2710 - val_loss: 4.3468 - val_acc: 0.2537\n",
      "Epoch 50/500\n",
      "128/128 [==============================] - 333s 3s/step - loss: 3.9334 - acc: 0.2725 - val_loss: 4.3271 - val_acc: 0.2557\n",
      "Epoch 51/500\n",
      "128/128 [==============================] - 335s 3s/step - loss: 3.9146 - acc: 0.2747 - val_loss: 4.3221 - val_acc: 0.2569\n",
      "Epoch 52/500\n",
      "128/128 [==============================] - 335s 3s/step - loss: 3.8940 - acc: 0.2765 - val_loss: 4.2983 - val_acc: 0.2581\n",
      "Epoch 53/500\n",
      "128/128 [==============================] - 416s 3s/step - loss: 3.8765 - acc: 0.2779 - val_loss: 4.3000 - val_acc: 0.2587\n",
      "Epoch 54/500\n",
      "128/128 [==============================] - 334s 3s/step - loss: 3.8589 - acc: 0.2796 - val_loss: 4.3021 - val_acc: 0.2594\n",
      "Epoch 55/500\n",
      "128/128 [==============================] - 299s 2s/step - loss: 3.8402 - acc: 0.2809 - val_loss: 4.2946 - val_acc: 0.2604\n",
      "Epoch 56/500\n",
      "128/128 [==============================] - 347s 3s/step - loss: 3.8232 - acc: 0.2817 - val_loss: 4.2959 - val_acc: 0.2598\n",
      "Epoch 57/500\n",
      "128/128 [==============================] - 335s 3s/step - loss: 3.8046 - acc: 0.2847 - val_loss: 4.2700 - val_acc: 0.2619\n",
      "Epoch 58/500\n",
      "128/128 [==============================] - 323s 3s/step - loss: 3.7886 - acc: 0.2854 - val_loss: 4.2706 - val_acc: 0.2619\n",
      "Epoch 59/500\n",
      "128/128 [==============================] - 298s 2s/step - loss: 3.7708 - acc: 0.2873 - val_loss: 4.2679 - val_acc: 0.2624\n",
      "Epoch 60/500\n",
      "128/128 [==============================] - 335s 3s/step - loss: 3.7530 - acc: 0.2894 - val_loss: 4.2560 - val_acc: 0.2646\n",
      "Epoch 61/500\n",
      "128/128 [==============================] - 463s 4s/step - loss: 3.7346 - acc: 0.2910 - val_loss: 4.2398 - val_acc: 0.2657\n",
      "Epoch 62/500\n",
      "128/128 [==============================] - 307s 2s/step - loss: 3.7211 - acc: 0.2918 - val_loss: 4.2472 - val_acc: 0.2671\n",
      "Epoch 63/500\n",
      "128/128 [==============================] - 276s 2s/step - loss: 3.7026 - acc: 0.2938 - val_loss: 4.2472 - val_acc: 0.2664\n",
      "Epoch 64/500\n",
      "128/128 [==============================] - 282s 2s/step - loss: 3.6862 - acc: 0.2954 - val_loss: 4.2432 - val_acc: 0.2662\n",
      "Epoch 65/500\n",
      "128/128 [==============================] - 271s 2s/step - loss: 3.6701 - acc: 0.2967 - val_loss: 4.2276 - val_acc: 0.2690\n",
      "Epoch 66/500\n",
      "128/128 [==============================] - 270s 2s/step - loss: 3.6547 - acc: 0.2981 - val_loss: 4.2225 - val_acc: 0.2682\n",
      "Epoch 67/500\n",
      "128/128 [==============================] - 268s 2s/step - loss: 3.6387 - acc: 0.3005 - val_loss: 4.2019 - val_acc: 0.2710\n",
      "Epoch 68/500\n",
      "128/128 [==============================] - 270s 2s/step - loss: 3.6226 - acc: 0.3014 - val_loss: 4.2103 - val_acc: 0.2697\n",
      "Epoch 69/500\n",
      "128/128 [==============================] - 269s 2s/step - loss: 3.6071 - acc: 0.3018 - val_loss: 4.2118 - val_acc: 0.2708\n",
      "Epoch 70/500\n",
      "128/128 [==============================] - 296s 2s/step - loss: 3.5908 - acc: 0.3041 - val_loss: 4.2091 - val_acc: 0.2690\n",
      "Epoch 71/500\n",
      "128/128 [==============================] - 345s 3s/step - loss: 3.5772 - acc: 0.3053 - val_loss: 4.2119 - val_acc: 0.2715\n",
      "Epoch 72/500\n",
      "128/128 [==============================] - 271s 2s/step - loss: 3.5612 - acc: 0.3079 - val_loss: 4.1867 - val_acc: 0.2740\n",
      "Epoch 73/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 3.5454 - acc: 0.3086 - val_loss: 4.1864 - val_acc: 0.2713\n",
      "Epoch 74/500\n",
      "128/128 [==============================] - 268s 2s/step - loss: 3.5321 - acc: 0.3090 - val_loss: 4.1884 - val_acc: 0.2718\n",
      "Epoch 75/500\n",
      "128/128 [==============================] - 272s 2s/step - loss: 3.5168 - acc: 0.3111 - val_loss: 4.1780 - val_acc: 0.2735\n",
      "Epoch 76/500\n",
      "128/128 [==============================] - 295s 2s/step - loss: 3.5003 - acc: 0.3124 - val_loss: 4.1623 - val_acc: 0.2733\n",
      "Epoch 77/500\n",
      "128/128 [==============================] - 284s 2s/step - loss: 3.4850 - acc: 0.3139 - val_loss: 4.1729 - val_acc: 0.2756\n",
      "Epoch 78/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 3.4709 - acc: 0.3162 - val_loss: 4.1747 - val_acc: 0.2743\n",
      "Epoch 79/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 3.4574 - acc: 0.3170 - val_loss: 4.1721 - val_acc: 0.2743\n",
      "Epoch 80/500\n",
      "128/128 [==============================] - 267s 2s/step - loss: 3.4433 - acc: 0.3188 - val_loss: 4.1564 - val_acc: 0.2771\n",
      "Epoch 81/500\n",
      "128/128 [==============================] - 265s 2s/step - loss: 3.4289 - acc: 0.3195 - val_loss: 4.1553 - val_acc: 0.2769\n",
      "Epoch 82/500\n",
      "128/128 [==============================] - 265s 2s/step - loss: 3.4149 - acc: 0.3203 - val_loss: 4.1322 - val_acc: 0.2782\n",
      "Epoch 83/500\n",
      "128/128 [==============================] - 262s 2s/step - loss: 3.4009 - acc: 0.3222 - val_loss: 4.1449 - val_acc: 0.2772\n",
      "Epoch 84/500\n",
      "128/128 [==============================] - 264s 2s/step - loss: 3.3883 - acc: 0.3243 - val_loss: 4.1503 - val_acc: 0.2748\n",
      "Epoch 85/500\n",
      "128/128 [==============================] - 262s 2s/step - loss: 3.3741 - acc: 0.3247 - val_loss: 4.1477 - val_acc: 0.2760\n",
      "Epoch 86/500\n",
      "128/128 [==============================] - 261s 2s/step - loss: 3.3618 - acc: 0.3261 - val_loss: 4.1521 - val_acc: 0.2784\n",
      "Epoch 87/500\n",
      "128/128 [==============================] - 262s 2s/step - loss: 3.3477 - acc: 0.3284 - val_loss: 4.1226 - val_acc: 0.2808\n",
      "Epoch 88/500\n",
      "128/128 [==============================] - 262s 2s/step - loss: 3.3342 - acc: 0.3288 - val_loss: 4.1263 - val_acc: 0.2806\n",
      "Epoch 89/500\n",
      "128/128 [==============================] - 261s 2s/step - loss: 3.3203 - acc: 0.3309 - val_loss: 4.1286 - val_acc: 0.2788\n",
      "Epoch 90/500\n",
      "128/128 [==============================] - 261s 2s/step - loss: 3.3084 - acc: 0.3323 - val_loss: 4.1200 - val_acc: 0.2804\n",
      "Epoch 91/500\n",
      "128/128 [==============================] - 270s 2s/step - loss: 3.2926 - acc: 0.3343 - val_loss: 4.1095 - val_acc: 0.2817\n",
      "Epoch 92/500\n",
      "128/128 [==============================] - 266s 2s/step - loss: 3.2819 - acc: 0.3354 - val_loss: 4.1184 - val_acc: 0.2835\n",
      "Epoch 93/500\n",
      "128/128 [==============================] - 268s 2s/step - loss: 3.2687 - acc: 0.3365 - val_loss: 4.1199 - val_acc: 0.2811\n",
      "Epoch 94/500\n",
      "128/128 [==============================] - 270s 2s/step - loss: 3.2572 - acc: 0.3371 - val_loss: 4.1197 - val_acc: 0.2815\n",
      "Epoch 95/500\n",
      "128/128 [==============================] - 268s 2s/step - loss: 3.2425 - acc: 0.3389 - val_loss: 4.1046 - val_acc: 0.2824\n",
      "Epoch 96/500\n",
      "128/128 [==============================] - 309s 2s/step - loss: 3.2323 - acc: 0.3396 - val_loss: 4.1066 - val_acc: 0.2835\n",
      "Epoch 97/500\n",
      "128/128 [==============================] - 435s 3s/step - loss: 3.2193 - acc: 0.3413 - val_loss: 4.0873 - val_acc: 0.2837\n",
      "Epoch 98/500\n",
      "128/128 [==============================] - 336s 3s/step - loss: 3.2074 - acc: 0.3422 - val_loss: 4.1043 - val_acc: 0.2836\n",
      "Epoch 99/500\n",
      "128/128 [==============================] - 417s 3s/step - loss: 3.1962 - acc: 0.3444 - val_loss: 4.1069 - val_acc: 0.2833\n",
      "Epoch 100/500\n",
      "128/128 [==============================] - 529s 4s/step - loss: 3.1835 - acc: 0.3446 - val_loss: 4.1092 - val_acc: 0.2835\n",
      "Epoch 101/500\n",
      "128/128 [==============================] - 416s 3s/step - loss: 3.1717 - acc: 0.3461 - val_loss: 4.1130 - val_acc: 0.2839\n",
      "Epoch 102/500\n",
      "128/128 [==============================] - 456s 4s/step - loss: 3.1594 - acc: 0.3472 - val_loss: 4.0843 - val_acc: 0.2881\n",
      "Epoch 103/500\n",
      "128/128 [==============================] - 451s 4s/step - loss: 3.1488 - acc: 0.3478 - val_loss: 4.0864 - val_acc: 0.2869\n",
      "Epoch 104/500\n",
      " 54/128 [===========>..................] - ETA: 3:35 - loss: 3.1320 - acc: 0.3492"
     ]
    }
   ],
   "source": [
    "model.fit(generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57152db7-a3f8-42d8-b446-5fbe07941c7c",
   "metadata": {},
   "source": [
    "# Always Remember to Save the Model so You Don't Need to Train Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "207cf3e6-ae65-474d-bfee-20bda0c5e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('model_with_attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420e04f-c754-43dd-8a37-9eb091cd8767",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_with_attention.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181556b9-82b8-4d47-9f60-82fff3ccc312",
   "metadata": {},
   "source": [
    "# This is the Inference Model in Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eabb9d5e-f0e6-4044-8327-019ec4f1b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models\n",
    "# For the encoder model, the outputs are the encoder outputs as well to be used in attention\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_states)\n",
    "\n",
    "# Decoder inference\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_length_src, latent_dim))\n",
    "\n",
    "# Get the embeddings and apply the decoder LSTM\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# Attention inference\n",
    "attention_result_inf = attention_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attention_result_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90c47e1b-ee9f-4005-b6db-d6affdf32d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loop to decode a batch of sequences\n",
    "def decode_sequence_batch(input_seqs):\n",
    "    # Encode the input as state vectors\n",
    "    encoder_outputs, states_value_h, states_value_c = encoder_model.predict(input_seqs)\n",
    "\n",
    "    # Create batch size empty target sequences\n",
    "    target_seqs = np.zeros((input_seqs.shape[0], 1))\n",
    "    # Populate the first character of target sequence with the start token for each one\n",
    "    target_seqs[:, 0] = target_token_index['START_']\n",
    "\n",
    "    decoded_sentences = [''] * input_seqs.shape[0]\n",
    "    stop_conditions = [False] * input_seqs.shape[0]\n",
    "\n",
    "    while not all(stop_conditions):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seqs] + [encoder_outputs, states_value_h, states_value_c])\n",
    "\n",
    "        # Sample tokens for each sequence in the batch\n",
    "        sampled_token_indices = np.argmax(output_tokens[:, -1, :], axis=1)\n",
    "        for i in range(input_seqs.shape[0]):\n",
    "            if not stop_conditions[i]:\n",
    "                sampled_token = reverse_target_char_index[sampled_token_indices[i]]\n",
    "                decoded_sentences[i] += ' ' + sampled_token\n",
    "\n",
    "                # Check for stop condition: either hit max length or find stop token\n",
    "                if (sampled_token == '_END' or len(decoded_sentences[i]) > max_length_tar):\n",
    "                    stop_conditions[i] = True\n",
    "\n",
    "        # Update the target sequence\n",
    "        target_seqs = np.array(sampled_token_indices)[:, np.newaxis]\n",
    "\n",
    "        # Update states\n",
    "        states_value_h, states_value_c = h, c\n",
    "\n",
    "    return decoded_sentences\n",
    "\n",
    "# Define a function to convert raw text to padded sequences\n",
    "def text_to_padded_sequences(texts, tokenizer, maxlen):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Function to translate new sentences that are not in the dataset\n",
    "def translate_new_sentences(sentences):\n",
    "    input_seqs = text_to_padded_sequences(sentences, input_tokenizer, max_length_src)\n",
    "    decoded_sentences = decode_sequence_batch(input_seqs)\n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c8e1d14-2baa-48d4-be49-804224593d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 553ms/step\n",
      "1/1 [==============================] - 1s 520ms/step\n",
      "1/1 [==============================] - 0s 493ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      " میں ابھی تک نہیں دے گا۔\n",
      " ہم نے بات کی۔ _END\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "new_sentences = ['This is a new sentence.', 'Another example sentence.']\n",
    "translations = translate_new_sentences(new_sentences)\n",
    "for translation in translations:\n",
    "    print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91da1841-6e5d-48ec-9e0a-ed7c3d8175d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      " مدد۔ _END\n"
     ]
    }
   ],
   "source": [
    "# Define the tokenizer for input sequences\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(lines.eng)\n",
    "\n",
    "# Function to translate new sentences that are not in the dataset\n",
    "def translate_new_sentences(sentences):\n",
    "    input_seqs = text_to_padded_sequences(sentences, input_tokenizer, max_length_src)\n",
    "    decoded_sentences = decode_sequence_batch(input_seqs)\n",
    "    return decoded_sentences\n",
    "\n",
    "# Example usage:\n",
    "new_sentences = ['hi']\n",
    "translations = translate_new_sentences(new_sentences)\n",
    "for translation in translations:\n",
    "    print(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bb50e5f-4c5e-4266-8a7d-2633cb17ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode sample sequences function using attention\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors and retrieve encoder outputs\n",
    "    encoder_outputs, h, c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with the start token\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences (assuming a batch of size 1)\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [encoder_outputs, h, c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_char == '_END'):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence with the sampled token\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fcd9de5-fdbc-4ec3-b06a-1907b5ec403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Input English sentence: the last bus had already gone when i got to the bus stop\n",
      "Actual Urdu Translation:  میرے بس سٹاپ پہنچنے تک آخری بس نکل چکی تھی۔ \n",
      "Predicted Urdu Translation:  میرے بس سٹاپ پہنچنے تک آخری بس نکل چکی تھی۔ \n"
     ]
    }
   ],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1\n",
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Urdu Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Urdu Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "570036b0-7250-42db-b3a5-e8fa12a8992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "897f7ef6-7523-44b7-88b9-b6cdae03faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Input English sentence: the snow melted away when spring came\n",
      "Actual Urdu Translation:  بہار کا موسم آتے ہی برف پگھل گئی۔ \n",
      "Predicted Urdu Translation:  وہ خیال میں کوئی نہیں سکتا۔ \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Urdu Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Urdu Translation:', decoded_sentence[:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
